{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10947926,"sourceType":"datasetVersion","datasetId":6809663}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install streamlit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:35:25.726165Z","iopub.execute_input":"2025-03-07T15:35:25.726478Z","iopub.status.idle":"2025-03-07T15:35:31.550662Z","shell.execute_reply.started":"2025-03-07T15:35:25.726448Z","shell.execute_reply":"2025-03-07T15:35:31.549865Z"}},"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Downloading streamlit-1.43.0-py2.py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.43.0-py2.py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.43.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport math\nimport time\nimport streamlit as st\nfrom transformers import BertTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:35:34.703557Z","iopub.execute_input":"2025-03-07T15:35:34.703863Z","iopub.status.idle":"2025-03-07T15:35:40.594189Z","shell.execute_reply.started":"2025-03-07T15:35:34.703838Z","shell.execute_reply":"2025-03-07T15:35:40.593335Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n\n# --- Data Loading and Preprocessing ---\n\ndef load_spoc_data_reversed(file_path, code_col='code', text_col='text'): # Reversed column order\n    \"\"\"Loads SPoC data from a TSV file, C++ as source, pseudocode as target.\"\"\"\n    df = pd.read_csv(file_path, sep='\\t')\n    # Filter out rows where either 'text' or 'code' is NaN\n    df_clean = df.dropna(subset=[text_col, code_col])\n    return df_clean[code_col].tolist(), df_clean[text_col].tolist() # Return code first, then text\n\n# Load BERT tokenizer (no change)\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nPAD_TOKEN = bert_tokenizer.pad_token\nUNK_TOKEN = bert_tokenizer.unk_token\nBOS_TOKEN = bert_tokenizer.cls_token\nEOS_TOKEN = bert_tokenizer.sep_token\n\n\ndef tokenize_and_numericalize_bert(texts, codes, tokenizer, max_len=128): # No change needed in tokenization function itself\n    \"\"\"Tokenizes, numericalizes, and pads text and code sequences using BERT tokenizer.\"\"\"\n    text_sequences_numericalized = []\n    code_sequences_numericalized = []\n\n    for text, code in zip(texts, codes): # 'texts' will now be C++ code, 'codes' will be pseudocode\n        encoded_text = tokenizer.encode(\n            BOS_TOKEN + \" \" + text + \" \" + EOS_TOKEN,\n            add_special_tokens=False,\n            max_length=max_len,\n            truncation=True,\n            padding='max_length'\n        )\n        encoded_code = tokenizer.encode(\n            BOS_TOKEN + \" \" + code + \" \" + EOS_TOKEN,\n            add_special_tokens=False,\n            max_length=max_len,\n            truncation=True,\n            padding='max_length'\n        )\n\n        text_sequences_numericalized.append(torch.tensor(encoded_text))\n        code_sequences_numericalized.append(torch.tensor(encoded_code))\n\n    text_sequences_padded = torch.stack(text_sequences_numericalized)\n    code_sequences_padded = torch.stack(code_sequences_numericalized)\n\n    return text_sequences_padded, code_sequences_padded\n\nclass SPOCDataset(Dataset): # No change needed for Dataset class\n    def __init__(self, text_sequences, code_sequences):\n        self.text_sequences = text_sequences\n        self.code_sequences = code_sequences\n\n    def __len__(self):\n        return len(self.text_sequences)\n\n    def __getitem__(self, idx):\n        return self.text_sequences[idx], self.code_sequences[idx]\n\n\n# --- Main Data Preparation (Reversed) ---\ntrain_file_path = \"/kaggle/input/spoc-train/spoc-train.tsv\"\ntrain_codes, train_texts = load_spoc_data_reversed(train_file_path) # Load with reversed order\n\n# Tokenize and numericalize using BERT tokenizer - No change needed here, roles swapped in loaded data\nMAX_SEQ_LEN = 128\ntrain_code_numericalized, train_text_numericalized = tokenize_and_numericalize_bert( # Swapped variable names to reflect new roles\n    train_codes, train_texts, bert_tokenizer, max_len=MAX_SEQ_LEN # 'train_codes' is now source, 'train_texts' is target\n)\n\n# Create Dataset and DataLoader - No change needed\ntrain_dataset = SPOCDataset(train_code_numericalized, train_text_numericalized) # Swapped dataset inputs\nBATCH_SIZE = 128\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n\n# --- Vocabulary Sizes (BERT tokenizer - no change in size, just labels) ---\nCODE_VOCAB_SIZE = bert_tokenizer.vocab_size # Now C++ vocab size (source)\nTEXT_VOCAB_SIZE = bert_tokenizer.vocab_size # Now Pseudocode vocab size (target)\n\n\nprint(f\"C++ Vocabulary Size (BERT): {CODE_VOCAB_SIZE}\") # Updated print statement\nprint(f\"Pseudocode Vocabulary Size (BERT): {TEXT_VOCAB_SIZE}\") # Updated print statement\nprint(f\"Number of training examples: {len(train_dataset)}\")\n\ntext_batch, code_batch = next(iter(train_dataloader)) # 'text_batch' is now C++ batch, 'code_batch' is pseudocode batch\nprint(\"Example C++ batch shape:\", text_batch.shape) # Updated print statement\nprint(\"Example pseudocode batch shape:\", code_batch.shape) # Updated print statement\n\n\n# --- Placeholders for Tokenizers and Vocabs (Roles Swapped) ---\ncpp_tokenizer = bert_tokenizer # C++ tokenizer (source)\nps_tokenizer = bert_tokenizer # Pseudocode tokenizer (target)\ncpp_vocab_size = CODE_VOCAB_SIZE # Source vocab size\nps_vocab_size = TEXT_VOCAB_SIZE # Target vocab size\n\n\n# --- Device Configuration (No change) ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n# --- Transformer Model Implementation (No change - same model architecture) ---\nclass Transformer(nn.Module): # ... (rest of Transformer, MHA, EncoderLayer, DecoderLayer, PFFN classes - same as before)\n    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, num_heads=8,\n                 num_layers=3, d_ff=512, dropout=0.1, max_len=128):\n        super(Transformer, self).__init__()\n\n        # Embedding layers\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model) # Source embedding (C++)\n        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model) # Target embedding (Pseudocode)\n        self.dropout = nn.Dropout(dropout)\n        # ... (rest of Transformer __init__, make_src_mask, make_trg_mask, forward methods - same as before)\n        # Positional encoding\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n        # Encoder and decoder layers\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n        ])\n\n        # Output layer\n        self.output_layer = nn.Linear(d_model, trg_vocab_size)\n\n        # Other parameters\n        self.scale = math.sqrt(d_model)\n        self.src_pad_idx = bert_tokenizer.pad_token_id # Use BERT pad token id\n        self.trg_pad_idx = bert_tokenizer.pad_token_id # Use BERT pad token id\n\n\n    def make_src_mask(self, src):\n        # Create mask for padding in source (1 for tokens, 0 for padding)\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n        return src_mask\n\n    def make_trg_mask(self, trg):\n        # Create mask for padding and look-ahead\n        trg_len = trg.shape[1]\n\n        # Padding mask (1 for tokens, 0 for padding)\n        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n\n        # Look-ahead mask (lower triangular matrix)\n        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n        trg_sub_mask = trg_sub_mask.unsqueeze(0).unsqueeze(0)\n\n        # Combine masks\n        trg_mask = trg_pad_mask & trg_sub_mask\n\n        return trg_mask\n\n    def forward(self, src, trg):\n        # Create masks\n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n\n        # Source embedding and positional encoding\n        src = self.src_embedding(src) * self.scale\n        src = src + self.pe[:, :src.size(1)].to(src.device)\n        src = self.dropout(src)\n\n        # Encoder\n        enc_output = src\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        # Target embedding and positional encoding\n        trg = self.trg_embedding(trg) * self.scale\n        trg = trg + self.pe[:, :trg.size(1)].to(trg.device)\n        trg = self.dropout(trg)\n\n        # Decoder\n        dec_output = trg\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, trg_mask, src_mask)\n\n        # Output\n        output = self.output_layer(dec_output)\n\n        return output\n\nclass MultiHeadAttention(nn.Module): # ... (MHA class - same as before)\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n        # ... (rest of MHA __init__, forward methods - same as before)\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n\n        self.fc = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.shape[0]\n\n        # Linear projections and reshape\n        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        # Attention\n        energy = torch.matmul(q, k.permute(0, 1, 3, 2)) / self.scale.to(q.device)\n\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n\n        attention = self.dropout(F.softmax(energy, dim=-1))\n        output = torch.matmul(attention, v)\n\n        # Reshape and concat heads\n        output = output.permute(0, 2, 1, 3).contiguous()\n        output = output.view(batch_size, -1, self.d_model)\n\n        # Final linear projection\n        output = self.fc(output)\n\n        return output\n\nclass PositionwiseFeedforward(nn.Module): # ... (PFFN class - same as before)\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedforward, self).__init__()\n        # ... (rest of PFFN __init__, forward methods - same as before)\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n\n\nclass EncoderLayer(nn.Module): # ... (EncoderLayer class - same as before)\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        # ... (rest of EncoderLayer __init__, forward methods - same as before)\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # Self-attention with residual connection and layer norm\n        attn_out = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n\n        # Feed forward with residual connection and layer norm\n        ff_out = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_out))\n\n        return x\n\nclass DecoderLayer(nn.Module): # ... (DecoderLayer class - same as before)\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        # ... (rest of DecoderLayer __init__, forward methods - same as before)\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.enc_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, trg_mask, src_mask):\n        # Self-attention with residual connection and layer norm\n        self_attn_out = self.self_attn(x, x, x, trg_mask)\n        x = self.norm1(x + self.dropout(self_attn_out))\n\n        # Encoder-decoder attention with residual connection and layer norm\n        enc_attn_out = self.enc_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(enc_attn_out))\n\n        # Feed forward with residual connection and layer norm\n        ff_out = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_out))\n\n        return x\n\n\n# --- Model Initialization (Roles Swapped in vocab sizes) ---\nmodel = Transformer(\n    src_vocab_size=cpp_vocab_size, # C++ vocab size (source)\n    trg_vocab_size=ps_vocab_size, # Pseudocode vocab size (target)\n    d_model=256,\n    num_heads=8,\n    num_layers=3,\n    d_ff=512,\n    dropout=0.1\n).to(device)\nprint(f'Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters')\n\n\n# --- Training Setup (No change needed in training loop logic) ---\nLEARNING_RATE = 0.0001\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=cpp_tokenizer.pad_token_id) # Pad token ID is the same for both tokenizers\n\nNUM_EPOCHS = 20\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    for batch_idx, (src, trg) in enumerate(dataloader): # 'src' is now C++ batch, 'trg' is pseudocode batch\n        src, trg = src.to(device), trg.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(src, trg[:, :-1]) # 'src' is C++, 'trg' is pseudocode\n\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg_reshape = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg_reshape)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:35:57.485140Z","iopub.execute_input":"2025-03-07T15:35:57.485651Z","iopub.status.idle":"2025-03-07T15:38:05.516205Z","shell.execute_reply.started":"2025-03-07T15:35:57.485620Z","shell.execute_reply":"2025-03-07T15:38:05.515475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d9daa568ae4ebf8964921b717effbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de44ff7944034665a70743ed036c717f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb185593290f4a4680ba1c8b90c21c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f3b1fad2fb4c288b1e50d08dd239f5"}},"metadata":{}},{"name":"stdout","text":"C++ Vocabulary Size (BERT): 30522\nPseudocode Vocabulary Size (BERT): 30522\nNumber of training examples: 216225\nExample C++ batch shape: torch.Size([128, 128])\nExample pseudocode batch shape: torch.Size([128, 128])\nUsing device: cuda\nModel initialized with 27,425,082 parameters\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# *Training Code*","metadata":{}},{"cell_type":"code","source":"def train(model, train_dataloader, optimizer, criterion, num_epochs, device):\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n        end_time = time.time()\n        epoch_mins = int((end_time - start_time) / 60)\n        epoch_secs = int((end_time - start_time) - (epoch_mins * 60))\n\n        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n\n\n# --- Run Training ---\ntrain(model, train_dataloader, optimizer, criterion, NUM_EPOCHS, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:38:37.082725Z","iopub.execute_input":"2025-03-07T15:38:37.083070Z","iopub.status.idle":"2025-03-07T18:30:34.104501Z","shell.execute_reply.started":"2025-03-07T15:38:37.083044Z","shell.execute_reply":"2025-03-07T18:30:34.103744Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 | Time: 8m 37s\n\tTrain Loss: 2.363 | Train PPL:  10.618\nEpoch: 02 | Time: 8m 36s\n\tTrain Loss: 1.225 | Train PPL:   3.405\nEpoch: 03 | Time: 8m 36s\n\tTrain Loss: 1.018 | Train PPL:   2.767\nEpoch: 04 | Time: 8m 36s\n\tTrain Loss: 0.904 | Train PPL:   2.469\nEpoch: 05 | Time: 8m 36s\n\tTrain Loss: 0.829 | Train PPL:   2.290\nEpoch: 06 | Time: 8m 36s\n\tTrain Loss: 0.775 | Train PPL:   2.170\nEpoch: 07 | Time: 8m 36s\n\tTrain Loss: 0.733 | Train PPL:   2.081\nEpoch: 08 | Time: 8m 35s\n\tTrain Loss: 0.701 | Train PPL:   2.016\nEpoch: 09 | Time: 8m 36s\n\tTrain Loss: 0.675 | Train PPL:   1.963\nEpoch: 10 | Time: 8m 36s\n\tTrain Loss: 0.652 | Train PPL:   1.919\nEpoch: 11 | Time: 8m 35s\n\tTrain Loss: 0.632 | Train PPL:   1.881\nEpoch: 12 | Time: 8m 35s\n\tTrain Loss: 0.615 | Train PPL:   1.850\nEpoch: 13 | Time: 8m 35s\n\tTrain Loss: 0.600 | Train PPL:   1.822\nEpoch: 14 | Time: 8m 35s\n\tTrain Loss: 0.587 | Train PPL:   1.798\nEpoch: 15 | Time: 8m 35s\n\tTrain Loss: 0.575 | Train PPL:   1.778\nEpoch: 16 | Time: 8m 35s\n\tTrain Loss: 0.564 | Train PPL:   1.758\nEpoch: 17 | Time: 8m 35s\n\tTrain Loss: 0.554 | Train PPL:   1.741\nEpoch: 18 | Time: 8m 35s\n\tTrain Loss: 0.546 | Train PPL:   1.726\nEpoch: 19 | Time: 8m 35s\n\tTrain Loss: 0.538 | Train PPL:   1.712\nEpoch: 20 | Time: 8m 35s\n\tTrain Loss: 0.529 | Train PPL:   1.698\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Save the trained model and related information (Roles Swapped in names) ---\nMODEL_SAVE_PATH = 'cpp_to_pseudocode_model_bert_20epoch.pth' # Updated model save path\nTOKENIZER_INFO_PATH = \"tokenizer_info_cpp_to_ps_bert_20epoch.txt\" # Updated tokenizer info path\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'src_vocab_size': cpp_vocab_size, # C++ vocab size\n    'trg_vocab_size': ps_vocab_size, # Pseudocode vocab size\n    'max_length': MAX_SEQ_LEN,\n    'd_model': 256,\n    'num_heads': 8,\n    'num_layers': 3,\n    'd_ff': 512,\n    'dropout': 0.1\n}, MODEL_SAVE_PATH)\n\nprint(f\"Model saved successfully to: {MODEL_SAVE_PATH}\")\nwith open(TOKENIZER_INFO_PATH, \"w\") as f:\n    f.write(f\"C++ vocab size (BERT): {cpp_vocab_size}\\n\") # Updated tokenizer info saving\n    f.write(f\"Pseudocode vocab size (BERT): {ps_vocab_size}\\n\") # Updated tokenizer info saving\n    f.write(f\"Tokenizer type: bert-base-uncased\\n\")\n\nprint(f\"Tokenizer information saved to: {TOKENIZER_INFO_PATH}\")\n\n\n\n\n\n# --- Streamlit Deployment (Will be addressed later) ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:35:49.729929Z","iopub.execute_input":"2025-03-07T18:35:49.730247Z","iopub.status.idle":"2025-03-07T18:35:50.191943Z","shell.execute_reply.started":"2025-03-07T18:35:49.730220Z","shell.execute_reply":"2025-03-07T18:35:50.191071Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully to: cpp_to_pseudocode_model_bert_20epoch.pth\nTokenizer information saved to: tokenizer_info_cpp_to_ps_bert_20epoch.txt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T08:41:33.304161Z","iopub.execute_input":"2025-03-07T08:41:33.304679Z","iopub.status.idle":"2025-03-07T08:41:38.425344Z","shell.execute_reply.started":"2025-03-07T08:41:33.304638Z","shell.execute_reply":"2025-03-07T08:41:38.423826Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport math\nimport time\nfrom transformers import BertTokenizer\n\n# --- Testing Code ---\n\n# --- Load Tokenizer and Model Info ---\nMODEL_SAVE_PATH = 'cpp_to_pseudocode_model_bert_20epoch.pth'\nTOKENIZER_INFO_PATH = \"tokenizer_info_cpp_to_ps_bert_20epoch.txt\"\n\ndef load_model_and_tokenizer(model_path, tokenizer_info_path):\n    \"\"\"Loads the trained model, tokenizer, and related information.\"\"\"\n    checkpoint = torch.load(model_path, map_location=torch.device('cpu')) # Load to CPU first\n    model_info = {\n        'src_vocab_size': checkpoint['src_vocab_size'],\n        'trg_vocab_size': checkpoint['trg_vocab_size'],\n        'max_length': checkpoint['max_length'],\n        'd_model': checkpoint['d_model'],\n        'num_heads': checkpoint['num_heads'],\n        'num_layers': checkpoint['num_layers'],\n        'd_ff': checkpoint['d_ff'],\n        'dropout': checkpoint['dropout']\n    }\n\n    # Initialize model\n    # Corrected class name here: Transformers -> Transformer\n    model = Transformer(\n        src_vocab_size=model_info['src_vocab_size'],\n        trg_vocab_size=model_info['trg_vocab_size'],\n        d_model=model_info['d_model'],\n        num_heads=model_info['num_heads'],\n        num_layers=model_info['num_layers'],\n        d_ff=model_info['d_ff'],\n        dropout=model_info['dropout'],\n        max_len=model_info['max_length'] # Pass max_len to the model init\n    )\n    model.load_state_dict(checkpoint['model_state_dict']) # Load model weights\n\n    # Load tokenizer info (for verification, though BERT tokenizer is fixed)\n    with open(tokenizer_info_path, \"r\") as f:\n        tokenizer_lines = f.readlines()\n        cpp_vocab_size_loaded = int(tokenizer_lines[0].split(\":\")[1].strip())\n        ps_vocab_size_loaded = int(tokenizer_lines[1].split(\":\")[1].strip())\n        tokenizer_type_loaded = tokenizer_lines[2].split(\":\")[1].strip()\n\n    # Load BERT tokenizer (ensure it's the same as training)\n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    PAD_TOKEN = bert_tokenizer.pad_token\n    UNK_TOKEN = bert_tokenizer.unk_token\n    BOS_TOKEN = bert_tokenizer.cls_token\n    EOS_TOKEN = bert_tokenizer.sep_token\n\n    print(\"Loaded Tokenizer Info:\")\n    print(f\"C++ vocab size: {cpp_vocab_size_loaded}\")\n    print(f\"Pseudocode vocab size: {ps_vocab_size_loaded}\")\n    print(f\"Tokenizer type: {tokenizer_type_loaded}\")\n\n    return model, bert_tokenizer, bert_tokenizer, model_info['max_length'] # Return model, cpp_tokenizer, ps_tokenizer, max_len\n\n# --- Tokenize and Numericalize for Testing ---\ndef tokenize_and_numericalize_test_bert(text, tokenizer, max_len=128):\n    \"\"\"Tokenizes, numericalizes, and pads a single text sequence for testing using BERT tokenizer.\"\"\"\n    encoded_text = tokenizer.encode(\n        BOS_TOKEN + \" \" + text + \" \" + EOS_TOKEN,\n        add_special_tokens=False,\n        max_length=max_len,\n        truncation=True,\n        padding='max_length'\n    )\n    return torch.tensor(encoded_text).unsqueeze(0) # Add batch dimension\n\n\n# --- Translation / Decoding Function ---\ndef translate_code_to_pseudocode_bert(model, src_sequence, max_len, device, tokenizer):\n    \"\"\"Translates C++ code to pseudocode using the trained Transformer model and BERT tokenizer.\"\"\"\n    model.eval() # Set model to evaluation mode\n    src_sequence = src_sequence.to(device)\n\n    src_mask = model.make_src_mask(src_sequence)\n\n    with torch.no_grad():\n        enc_output = model.forward(src_sequence, src_sequence)[:, :1, :] # Only encode once, dummy target\n    # Initialize target sequence with BOS_TOKEN index\n    trg_indexes = [tokenizer.cls_token_id] # Start with BOS token\n    for _ in range(max_len):\n        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device) # unsqueeze to add batch dim\n        trg_mask = model.make_trg_mask(trg_tensor)\n\n        with torch.no_grad():\n            output = model.forward(src_sequence, trg_tensor) # Re-use enc_output, dummy src\n        pred_token = output.argmax(2)[:, -1].item() # Get predicted token index from last position\n\n        trg_indexes.append(pred_token)\n        if pred_token == tokenizer.sep_token_id: # EOS_TOKEN\n            break\n\n    # Decode the numericalized pseudocode sequence back to text\n    trg_tokens = tokenizer.convert_ids_to_tokens(trg_indexes)\n    # Filter out special tokens and join\n    translated_text = tokenizer.convert_tokens_to_string(trg_tokens[1:-1]) # Remove BOS and EOS\n\n    return translated_text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:07:12.853199Z","iopub.execute_input":"2025-03-07T19:07:12.853476Z","iopub.status.idle":"2025-03-07T19:07:12.864558Z","shell.execute_reply.started":"2025-03-07T19:07:12.853454Z","shell.execute_reply":"2025-03-07T19:07:12.863582Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# --- Example Usage and Testing ---\nif __name__ == '__main__':\n    # --- Device Configuration ---\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device for testing: {device}\")\n\n    # Load the trained model and tokenizer\n    model, cpp_tokenizer, ps_tokenizer, max_len = load_model_and_tokenizer(MODEL_SAVE_PATH, TOKENIZER_INFO_PATH)\n    model = model.to(device) # Move model to the device\n\n    # --- Example C++ Code Snippets for Testing ---\n    test_cpp_examples = [\"for(int i = 0; i < n; i++) { cout<< ii;}\"]\n\n    # --- Translate and Print Results ---\n    print(\"\\n--- Testing Translations ---\")\n    for cpp_code in test_cpp_examples:\n        numericalized_cpp = tokenize_and_numericalize_test_bert(cpp_code, cpp_tokenizer, max_len)\n        predicted_pseudocode = translate_code_to_pseudocode_bert(model, numericalized_cpp, max_len, device, ps_tokenizer)\n\n        print(f\"\\nC++ Code:\\n`c++\\n{cpp_code}\\n`\")\n        print(f\"Predicted Pseudocode:\\n`pseudocode\\n{predicted_pseudocode}\\n`\")\n        print(\"-\" * 50)\n\n    print(\"\\nTesting complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:29:28.148394Z","iopub.execute_input":"2025-03-07T19:29:28.148722Z","iopub.status.idle":"2025-03-07T19:29:29.428565Z","shell.execute_reply.started":"2025-03-07T19:29:28.148695Z","shell.execute_reply":"2025-03-07T19:29:29.427788Z"}},"outputs":[{"name":"stdout","text":"Using device for testing: cuda\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-5b89490e3ab0>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=torch.device('cpu')) # Load to CPU first\n","output_type":"stream"},{"name":"stdout","text":"Loaded Tokenizer Info:\nC++ vocab size: 30522\nPseudocode vocab size: 30522\nTokenizer type: bert-base-uncased\n\n--- Testing Translations ---\n\nC++ Code:\n`c++\nfor(int i = 0; i < n; i++) { cout<< i*i;}\n`\nPredicted Pseudocode:\n`pseudocode\nfor i = 0 to n exclusive , print * i\n`\n--------------------------------------------------\n\nTesting complete.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}